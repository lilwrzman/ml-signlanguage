# ğŸ§© Sign Language Detection â€“ YOLO11x + OpenCV

![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)
![YOLO11x](https://img.shields.io/badge/YOLO11x-8.3.205-green?logo=ultralytics)
![Status](https://img.shields.io/badge/Setup%20Test-Passed-success?logo=anaconda)

### ğŸ‘‹ Overview
This project focuses on **real-time recognition of American Sign Language (ASL)** hand gestures using **YOLO11x** for object detection and **OpenCV** for video streaming and inference.  
It aims to translate hand signs (Aâ€“Z letters, space, period, and comma gestures) into readable text in real-time.

---

### ğŸ—ï¸ Project Architecture
Camera â†’ YOLO11x Detection â†’ Letter Buffer â†’ Word/Punctuation Logic â†’ Text Output (GUI)

---

### ğŸ§  Features
- ASL gesture detection (Aâ€“Z + punctuation)
- Real-time webcam inference with OpenCV
- Custom-trained YOLO11x model
- Streamlit/Tkinter GUI for live translation
- ONNX/TorchScript export for lightweight deployment

---

### âš™ï¸ Tech Stack
| Component | Technology |
|------------|-------------|
| Model | YOLO11x (Ultralytics) |
| Framework | PyTorch |
| Vision | OpenCV |
| UI | Streamlit / Tkinter |
| Dataset | ASL Kaggle Dataset |
| Language | Python 3.10 |

---

### ğŸ“¦ Setup Guide

**1. Clone the Repository**
```bash
git clone https://github.com/lilwrzman/ml-signlanguage.git
cd signlanguage-detection
```

**2. Create Conda Environment**
```bash
conda create -n signlanguage python=3.10 -y
conda activate signlanguage
```

**3. Install Dependencies**
```bash
pip install -r requirements.txt
```

**4. Run Initial Test Notebook**
To ensure all datasets and training results are stored inside this project (not in your AppData folder):
```bash
yolo settings datasets_dir="./data"
yolo settings runs_dir="./outputs"
```

**5. Run Initial Test Notebook**
Open the notebook:
```bash
notebooks/init_test.ipynb
```

This notebook performs a basic verification of:
- YOLO11x installation and version
- GPU/CUDA availability
- OpenCV camera initialization test
- Sample image inference

Run all cells in init_test.ipynb.
If all tests pass (model loads, GPU detected, and camera preview works), your environment is ready for dataset preparation and training.

---

### ğŸ“‚ Folder Structure
```graphql
ml-signlanguage/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ data.yaml # Dataset config file
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/ # YOLO-ready images 
â”‚   â””â”€â”€ raw/ # Original datasets (Kaggle ASL)and labels
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ checkpoints/ # Pretrained and trained YOLO weights
â”‚   â””â”€â”€ exports/ # ONNX / TorchScript exported models
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ init_test.ipynb # Init test
â”‚   â””â”€â”€ main.ipynb
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ logs/ # Training logs and metrics visualization
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ detection.py # Real-time gesture detection (webcam)
â”‚   â”œâ”€â”€ inference.py # Static image/video detection
â”‚   â”œâ”€â”€ app.py # Streamlit/Tkinter live demo
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ dataset_utils.py
â”‚       â”œâ”€â”€ preprocessing.py
â”‚       â””â”€â”€ postprocessing.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

### ğŸš€ Roadmap
- Project Initialization
- Dataset Preparation
- YOLO Training and Evaluation
- Real-Time Detection
- Word & Punctuation Logic
- UI + Deployment

---

### ğŸ§© Author

Egy Dya Hermawan | Application Developer / ML Enthusiast

ğŸ“ Indonesia | ğŸ“§ [Email](mailto:egydya.edh12@gmail.com) | ğŸ”— [LinkedIn](https://www.linkedin.com/in/egydyahermawan/)